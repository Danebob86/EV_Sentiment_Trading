{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries to be used in this notebook. We load the initial libraries at the beginning and keep on adding more libraries as we go along on the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import tweepy # for tweet mining\n",
    "from textblob import TextBlob # TextBlob - Python library for processing textual data\n",
    "from wordcloud import WordCloud # WordCloud - Python library for creating image wordclouds\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer #  lexicon and rule-based sentiment analysis tool\n",
    "import pandas as pd # for data manipulation and analysis\n",
    "import numpy as np # for working with arrays and carrying out mathematical operations.\n",
    "import re # In-built regular expressions library\n",
    "import os # to access environment from the operating system \n",
    "import string # Inbuilt string library\n",
    "import glob # to retrieve files/pathnames matching a specified pattern. \n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import alpaca_trade_api as tradeapi\n",
    "from datetime import datetime, timedelta\n",
    "import hvplot.pandas\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  # loading the  environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Environment from the operating system\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing Toolkit\n",
    "import nltk\n",
    "import spacy\n",
    "import emoji\n",
    "import stop_words\n",
    "# Tokenize - large quantity of text is divided into smaller parts \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk import pos_tag # For Parts of Speech tagging\n",
    "from collections import Counter # count the key-value pairs\n",
    "from spacy import displacy # dependency visualizer that show model's predictions\n",
    "nlp = spacy.load(\"en_core_web_sm\") # English pipeline optimization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Toolkit \n",
    "\n",
    "# (Ensembled Learning)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# (Linear Regression)\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# (Tensor Flow)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tweets Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Tweepy library for Python to scrape tweets. We created a developer account with Twitter to get the keys used below for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the API keys from env\n",
    "\n",
    "consumer_key = \"PHyNGsjBkAbUBTaW3EJV1jepw\"\n",
    "consumer_secret_key = \"RhAPZH5uTvcuodAQXWgsaCtnWV5gBIwbGuFNgB8asBdumUHBdY\"\n",
    "access_token = \"1023433478-n23JcqhTsmasEXAuFtJVU3casHoRjemeGR5c2A7\"\n",
    "access_token_secret = \"iXJ0XIB29VdNbObYmkNm1iWCZtfNaSnPFiRaNtBuiGM17\"\n",
    "\n",
    "# Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the API keys validate or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(consumer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(consumer_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-defined Functions for Tweets Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a functions for searching with different phrases, Example \"EV Stock\", \"Best EV Stock\", \"EV Stock Future\", \"Top EV Maker\". We do  different searches and each result store in a csv file. This is because we can be consistant with the data as we work along. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_csv(df, file_name):\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweets():\n",
    "    keyword = input(\"Please enter keyword or hashtag to search: \")\n",
    "    noOfTweet = int(input (\"Number of tweets to analyze: \"))\n",
    "    tweet_list = []\n",
    "    # Collect tweets using the Cursor object\n",
    "    tweets = tweepy.Cursor(api.search, \n",
    "                           q=keyword, \n",
    "                           keyword = keyword + \"  -filter:links AND -filter:retweets AND -filter:replies\", \n",
    "                          lang=\"en\",\n",
    "                          tweet_mode='extended' ).items(noOfTweet)\n",
    "     # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "    for tweet in tweets:\n",
    "        tweet_list.append([\n",
    "            tweet.id,\n",
    "            tweet.created_at,\n",
    "            tweet.full_text,\n",
    "            tweet.user.location,\n",
    "            tweet.retweet_count,\n",
    "            tweet.favorite_count\n",
    "        ])\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_list = getTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tweet_list_df = pd.DataFrame(tweet_list,columns=[\"Id\",\"Date\", \"Tweets\",\"Location\",\"Retweets\",\"Favorite\"])\n",
    "# tweet_list_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_list_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE DIFFERENT FILE NAME TO SAVE EACH SEARCH. OR else it will replace the previous data\n",
    "# write_df_to_csv(tweet_list_df,\"EV_best_2_tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read_csv with new search data to check. We do not need these lines .. \n",
    "## it is just to check if how may tweets we are getting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_tweet = pd.read_csv(\"EV_Stock_tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining all Tweets into single Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## commenting these lines as we got the tweets and save it to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/Users/syedahasan/Desktop/EV_Sentiment_Trading\"\n",
    "##all_files = glob.glob(path + \"/*.csv\")\n",
    "#all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = []\n",
    "\n",
    "#for filename in all_files:\n",
    "#    df = pd.read_csv(filename,  index_col=None, parse_dates=True,  header=0) # Convert each csv to a dataframe\n",
    "#    tweets.append(df)\n",
    "\n",
    "#tweets_df = pd.concat(tweets, axis=0, ignore_index=True) # Merge all dataframes\n",
    "#tweets_df['Date']= pd.to_datetime(tweets_df['Date'])\n",
    "#tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all tweets in one csv\n",
    "# write_df_to_csv(tweets_df,\"EV_ALL_TWEETS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"EV_ALL_TWEETS.csv\",  index_col=None, parse_dates=True,  header=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Location</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Favorite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1420544025631760384</td>\n",
       "      <td>2021-07-29</td>\n",
       "      <td>@ev_truths @alex_avoigt I think EV technology ...</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1420519469865177088</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>Compact International(1994) Wants to be Top EV...</td>\n",
       "      <td>U.S.A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1419726996104486919</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>RT @cliffski: Toyota are not much better than ...</td>\n",
       "      <td>Louisville, KY</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1419703662092472320</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>Toyota are not much better than volkswagen. Th...</td>\n",
       "      <td>None of your business</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1419665554038681606</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>GM China’s Venture Has Big Ambitions Beyond $4...</td>\n",
       "      <td>Vancouver and London</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id       Date  \\\n",
       "0  1420544025631760384 2021-07-29   \n",
       "1  1420519469865177088 2021-07-28   \n",
       "2  1419726996104486919 2021-07-26   \n",
       "3  1419703662092472320 2021-07-26   \n",
       "4  1419665554038681606 2021-07-26   \n",
       "\n",
       "                                              Tweets               Location  \\\n",
       "0  @ev_truths @alex_avoigt I think EV technology ...            Seattle, WA   \n",
       "1  Compact International(1994) Wants to be Top EV...                  U.S.A   \n",
       "2  RT @cliffski: Toyota are not much better than ...         Louisville, KY   \n",
       "3  Toyota are not much better than volkswagen. Th...  None of your business   \n",
       "4  GM China’s Venture Has Big Ambitions Beyond $4...   Vancouver and London   \n",
       "\n",
       "   Retweets  Favorite  \n",
       "0         0         0  \n",
       "1         0         0  \n",
       "2         2         0  \n",
       "3         2         4  \n",
       "4         0         0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['Date']= pd.to_datetime(tweets_df['Date']).dt.normalize() # changing date time to date\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                   int64\n",
       "Date        datetime64[ns]\n",
       "Tweets              object\n",
       "Location            object\n",
       "Retweets             int64\n",
       "Favorite             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe that is created on above, will be cleaned in this section. We are going to search for any duplication and will remove it. It is also important to mention that the Tweet ID was considered as the Primary key for all the dataframe. We are also going to replace the \"NaN\" values in Location column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12676, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get number of rows and columns\n",
    "tweets_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4786"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.duplicated(subset='Tweets').sum() # Check for duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate values\n",
    "tweets_df=tweets_df.drop_duplicates(subset='Tweets',keep=\"last\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7890, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape after dropping duplicates\n",
    "tweets_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id          False\n",
       "Date        False\n",
       "Tweets      False\n",
       "Location     True\n",
       "Retweets    False\n",
       "Favorite    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for \"NaN\" values\n",
    "tweets_df.isna().any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"NaN\" values with \"No Location\"\n",
    "tweets_df['Location']=tweets_df['Location'].fillna('No location') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id          False\n",
       "Date        False\n",
       "Tweets      False\n",
       "Location    False\n",
       "Retweets    False\n",
       "Favorite    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for \"NaN\" values again\n",
    "tweets_df.isna().any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Individual Tweer needed to be cleaned. We are using several functions to clean the tweet and to show the words used. We are extracting only the adjectives to a new column to view the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK list of stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization is a technique that transforms various morphologies of a word into its base form. \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# Instantiate the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    text = text.lower()  #has to be in place\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # Remove @mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove # symbol\n",
    "    text = re.sub(r'$', '', text)  # Remove $ symbol\n",
    "    text = re.sub(r'RT[\\s]+','', text) # Remove RT\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove urls\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # Remove hyperlink\n",
    "    text = re.sub(r':','', text)\n",
    "   \n",
    "    sw = set(stopwords.words('english'))\n",
    "    sw_addons = {'do', 'of', '$','r', '*,*','-',', ','``','\\'s','.','-','public','rt',\n",
    "                 'company', 'instead','&','’', 'in', 'car','they','\\'your', 'what', 'also', 'seeing','agreement',\n",
    "                 'time','today','ktown','pc','wts','105k','55k','ed','seeing','takeover','brokerages','rap','meet',\n",
    "                 'vehicle','market','year','news','price','med','lpo','make',\n",
    "                 'bakersfieldcart','takeaway','late','catching','lates','ranges'}\n",
    "    words = word_tokenize(text)\n",
    "    output = [x.lower() for x in words if x.lower() not in sw.union(sw_addons)]\n",
    "    # Remove punctuations\n",
    "    unpunctuated_words = [char for char in output if char not in string.punctuation]\n",
    "    unpunctuated_words = ' '.join(unpunctuated_words)\n",
    "    \n",
    "    #return output\n",
    "    \n",
    "    return \"\".join(unpunctuated_words)  # join words with a space in between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text): \n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(string):\n",
    "    \"\"\" Sanitize one string \"\"\"\n",
    "    emoticon_string = r\"\"\"\n",
    "        (?:\n",
    "          [<>]?\n",
    "          [:;=8]                     # eyes\n",
    "          [\\-o\\*\\']?                 # optional nose\n",
    "          [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "          |\n",
    "          [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "          [\\-o\\*\\']?                 # optional nose\n",
    "          [:;=8]                     # eyes\n",
    "          [<>]?\n",
    "        )\"\"\"\n",
    "    stopwords = [w.lower() for w in stop_words.get_stop_words('en')]\n",
    "    # remove graphical emoji\n",
    "    string = give_emoji_free_text(string) \n",
    "\n",
    "    # remove textual emoji\n",
    "    string = re.sub(emoticon_string,'',string)\n",
    "\n",
    "    # normalize to lowercase \n",
    "    string = string.lower()\n",
    "\n",
    "    # spacy tokenizer \n",
    "    string_split = [token.text for token in nlp(string)]\n",
    "\n",
    "    # in case the string is empty \n",
    "    if not string_split:\n",
    "        return '' \n",
    "\n",
    "    # join back to string \n",
    "    string = ' '.join(string_split)\n",
    "\n",
    "    # remove user \n",
    "    # assuming user has @ in front\n",
    "    string = re.sub(r\"\"\"(?:@[\\w_]+)\"\"\",'',string)\n",
    "\n",
    "    #remove # and @\n",
    "    for punc in '\":!@#':\n",
    "        string = string.replace(punc, '')\n",
    "\n",
    "    # remove 't.co/' links\n",
    "    string = re.sub(r'http//t.co\\/[^\\s]+', '', string, flags=re.MULTILINE)\n",
    "\n",
    "    # removing stop words \n",
    "    string = ' '.join([w for w in string.split() if w not in stopwords])\n",
    "    string = re.sub(r\"http\\S+\", \"\", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tweets_df['Tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sanitize(tweets_df['Tweets'][12671]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(sanitize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # remove one and two character words\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(lambda x: re.sub(r'\\b\\w{1,3}\\b', '', x))\n",
    "# remove punctuation\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(lambda x: re.sub('[^\\w\\s]', ' ', x))\n",
    "# remove numerical values\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(lambda x: re.sub(r'[0-9]+', '', x))\n",
    "# \\s+ means all empty space (\\n, \\r, \\t)\n",
    "tweets_df['Tweets'] = tweets_df['Tweets'].apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_df['Tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Processed_Tweets'] = tweets_df['Tweets'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tweets_df['Processed_Tweets'][12671])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_df['Processed_Tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def all_noun(tweet):\n",
    "    \"\"\"\n",
    "    This function retrieves all the nouns on the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (string): The text to analyze.\n",
    "        \n",
    "    Returns:\n",
    "        nouns (list): A list with all the nouns in the text.\n",
    "    \"\"\"\n",
    "   \n",
    "    tweet = word_tokenize(tweet)  # convert string to tokens\n",
    "    tweet = [word for (word, tag) in pos_tag(tweet)\n",
    "             if tag == \"NN\"]  # pos_tag module in NLTK library\n",
    "    return \" \".join(tweet)  # join words with a space in between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def all_adjectives(tweet):\n",
    "    \"\"\"\n",
    "    This function retrieves all the adjectives on the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (string): The text to analyze.\n",
    "        \n",
    "    Returns:\n",
    "        adjs (list): A list with all the adjectives in the text.\n",
    "    \"\"\"\n",
    "   \n",
    "    tweet = word_tokenize(tweet)  # convert string to tokens\n",
    "    tweet = [word for (word, tag) in pos_tag(tweet)\n",
    "             if tag == \"JJ\"]  # pos_tag module in NLTK library\n",
    "    return \" \".join(tweet)  # join words with a space in between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all_noun function to the new 'Processed Tweets' column to generate a new column called 'Tweets_Nouns'\n",
    "tweets_df['Tweets_Nouns'] = tweets_df['Processed_Tweets'].apply(all_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Tweets_Adjectives'] = tweets_df['Processed_Tweets'].apply(all_adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(t, w):\n",
    "    \"\"\"\n",
    "    This function counts the occurrences of a word in a text.\n",
    "    \n",
    "    Args:\n",
    "        text (string): The text where word counts will be analyzed.\n",
    "        word (string): The word to look into the text.\n",
    "        \n",
    "    Returns:\n",
    "        word_count (int): The counts of the word in the given text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the word_tokenize module from NLTK to tokenize the text\n",
    "    tok = word_tokenize(t)\n",
    "    \n",
    "    # Create a list with all the tokens retrieved from the text\n",
    "    tok = [w.lower() for w in tok]\n",
    "    \n",
    "    # Count the occurrences of the word in the text\n",
    "    word_count = tok.count(w)\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return words to their base form using Lemmatizer\n",
    "# ref: https://jess-analytics.medium.com/\n",
    "def preprocessTweetsSentiments(tweet):\n",
    "    \n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    lemmatizer = WordNetLemmatizer() # instatiate an object WordNetLemmatizer Class\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in tweet_tokens]\n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessTweetsSentiments function to the 'Processed Tweets' column to generate a new column\n",
    "tweets_df['Tweets_Lemmatize'] = tweets_df['Processed_Tweets'].apply(preprocessTweetsSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tweets into one long string with each word separate with a \"space\"\n",
    "tweets_long_string = tweets_df['Tweets_Adjectives'].tolist()\n",
    "tweets_long_string = \" \".join(tweets_long_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # for opening, manipulating, and saving many different image file \n",
    "# Import Twitter Logo\n",
    "image = np.array(Image.open('twitter.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to generate the blue colour for the Word CLoud\n",
    "def blue_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n",
    "    return \"hsl(210, 100%%, %d%%)\" % random.randint(50, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # generating random numbers\n",
    "# Instantiate the Twitter word cloud object\n",
    "# ref: https://amueller.github.io/word_cloud/auto_examples/masked.html\n",
    "twitter_wc = WordCloud(background_color='white', max_words=1500, mask=image)\n",
    "\n",
    "# generate the word cloud\n",
    "twitter_wc.generate(tweets_long_string)\n",
    "fig = plt.figure() # Instantiate the figure object\n",
    "fig.set_figwidth(14) # set width\n",
    "fig.set_figheight(18) # set height\n",
    "\n",
    "# display the word cloud\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(14)  # set width\n",
    "fig.set_figheight(18)  # set height\n",
    "\n",
    "plt.imshow(twitter_wc.recolor(color_func=blue_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all words into a list\n",
    "tweets_long_string = tweets_df['Tweets_Adjectives'].tolist()\n",
    "tweets_list=[]\n",
    "for item in tweets_long_string:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        tweets_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Built-in Python Collections module to determine Word frequency\n",
    "counts = Counter(tweets_list)\n",
    "df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "df.columns = ['Words', 'Count']\n",
    "df.sort_values(by='Count', ascending=False, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the aim was to undertand the sentiment of the Twitter Users while describe Electric Vehicle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment calculation based on compound score\n",
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return int(1) if (result == 1) else int(0) if result == 0 else int(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scores dictionaries\n",
    "tweet_sent = {\n",
    "    \"tweet\": [],\n",
    "    \"compound\": [],\n",
    "    \"positive\": [],\n",
    "    \"neutral\": [],\n",
    "    \"negative\": [],\n",
    "    \"sentiment\": [],\n",
    "}\n",
    "\n",
    "# Get sentiment for the tweet\n",
    "# iterrows() method is used to iterate across the dataframe \n",
    "for index, row in tweets_df.iterrows():\n",
    "    try:\n",
    "        # Sentiment scoring \n",
    "        tweet_sentiment = analyzer.polarity_scores(row[\"Processed_Tweets\"])  # VADER sentiment scores are retrieved for tweets\n",
    "        tweet_sent[\"tweet\"].append(row[\"Processed_Tweets\"])\n",
    "        tweet_sent[\"compound\"].append(tweet_sentiment[\"compound\"])\n",
    "        tweet_sent[\"positive\"].append(tweet_sentiment[\"pos\"])\n",
    "        tweet_sent[\"neutral\"].append(tweet_sentiment[\"neu\"])\n",
    "        tweet_sent[\"negative\"].append(tweet_sentiment[\"neg\"])\n",
    "        tweet_sent[\"sentiment\"].append(get_sentiment(tweet_sentiment[\"compound\"]))\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# Attaching sentiment columns to the News DataFrame\n",
    "\n",
    "tweet_sentiment_df = pd.DataFrame(tweet_sent)\n",
    "tweet_sentiment_df2 = tweets_df[\"Processed_Tweets\"].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment_df2 = tweet_sentiment_df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweet_sentiment_df2[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_date_df = tweets_df[[\"Date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_date_df.columns = [\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_date_df = tweet_date_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_date_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweet_date_df[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment_df2 = tweet_sentiment_df2.rename(columns={\"0\":\"polarity\",\"1\":\"subjectivity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment_df2.columns = [\"polarity\",\"subjectivity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_sentiment_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment_final = pd.concat([tweet_sentiment_df, tweet_sentiment_df2,tweet_date_df],axis=1,join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_sentiment_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ref : https://towardsdatascience.com/step-by-step-twitter-sentiment-analysis-in-python-d6f650ade58d\n",
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=[\"Total\",\"Percentage\"])\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tweet_sentiment_final,\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pichart = count_values_in_column(tweet_sentiment_df,\"sentiment\")\n",
    "names= pichart.index\n",
    "size=pichart[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color=\"white\")\n",
    "plt.pie(size, labels=names, colors=[\"green\",\"yellow\",\"red\"])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the TF-IDF\n",
    "# A TfidfVectorizer() instance is created by passing the stopwords in English as a parameter.\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X_tweets = vectorizer.fit_transform(tweets_df['Tweets_Lemmatize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting matrix info\n",
    "print(f\"Matrix shape: {X_tweets.shape}\")\n",
    "print(f\"Total number of tweets: {X_tweets.shape[0]}\")\n",
    "print(f\"Total number of unique words (tokens): {X_tweets.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve words list from tweets\n",
    "words_tweets = vectorizer.get_feature_names()\n",
    "#print(words_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the TF-IDF weight of each word in corpus as DataFrame\n",
    "# The mean value of the TF–IDF for each term is used to create the DataFrame.\n",
    "words_tweets_df = pd.DataFrame(\n",
    "    list(zip(words_tweets, np.ravel(X_tweets.mean(axis=0)))), columns=[\"Word\", \"TF-IDF\"]\n",
    ")\n",
    "\n",
    "words_tweets_df = words_tweets_df.sort_values(by=[\"TF-IDF\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tweets_df = words_tweets_df[:7890]\n",
    "words_tweets_df = words_tweets_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tweets_df.columns = [\"ID\", \"Word\", \"TFIDF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tweets_df = words_tweets_df.drop(columns=[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(tweets,ngram_range,n=None):\n",
    "    countVectorizer = CountVectorizer(ngram_range=ngram_range,stop_words = \"english\").fit(tweets)\n",
    "    bag_of_words = countVectorizer.transform(tweets)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in countVectorizer.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "#n2_bigram\n",
    "n2_bigrams = get_top_n_gram(tweets_df[\"Processed_Tweets\"],(2,2),20)\n",
    "ndf = pd.DataFrame(n2_bigrams, columns=[\"Bigrams\",\"Count\"]).set_index('Bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_string = ''\n",
    "for x in tweets_df[\"Processed_Tweets\"]:\n",
    "    tweet_string += ' ' + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(tweet_string)\n",
    "\n",
    "# Render NER visualization with displacy to determine entities for extraction\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract geopolitical and organizational entities\n",
    "entities = [x.text for x in doc.ents if x.label_ in ['GPE', 'ORG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower and join each entity for word cloud creation\n",
    "entities = [i.lower().replace(' ', '_') for i in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wordcloud\n",
    "wc = WordCloud().generate(' '.join(entities))\n",
    "plt.axis('off')\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Tone Analyzer response is given in JSON format, \n",
    "# so the json_normalize function is imported from Pandas to transform the JSON response to a DataFrame.\n",
    "from pandas import json_normalize\n",
    "\n",
    "# ToneAnalyzerV3 is the main library to access to the Tone Analyzer via Python.\n",
    "from ibm_watson import ToneAnalyzerV3\n",
    "\n",
    "# IAMAuthenticator is used to authenticate your Python application to access the IBM cloud services.\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Tone Analyzer API Key and URL\n",
    "tone_api = os.getenv(\"IBM_TONE_API\")\n",
    "tone_url = os.getenv(\"IBM_TONE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tone Analyser Client\n",
    "\n",
    "# Create authentication object\n",
    "authenticator = IAMAuthenticator(tone_api)\n",
    "\n",
    "# Create tone_analyzer instance\n",
    "tone_analyzer = ToneAnalyzerV3(\n",
    "    version=\"2017-09-21\",\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "# Set the service endpoint\n",
    "tone_analyzer.set_service_url(tone_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sh = tweets_df.iloc[:1000]\n",
    "tweets_sh = tweets_sh.drop_duplicates('Processed_Tweets', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sh_string = ''\n",
    "for x in tweets_sh['Processed_Tweets']:\n",
    "    tweet_sh_string += '. ' + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the text's tone with the 'tone()' method.\n",
    "\n",
    "# tone(): The general tone analysis, aimed to score tone on short text\n",
    "# (such as reviews, emails, or social media) or even larger texts (such as articles or blog post)\n",
    "\n",
    "# it only needs to receive a text to score, however, additional parameters could be used\n",
    "# https://cloud.ibm.com/apidocs/tone-analyzer?code=python#data-handling\n",
    "\n",
    "tone_analysis = tone_analyzer.tone( \n",
    "    {\"text\": tweet_sh_string},\n",
    "    content_type=\"application/json\",\n",
    "    content_language=\"en\",\n",
    "    accept_language=\"en\",\n",
    ").get_result()\n",
    "\n",
    "# Display tone analysis results\n",
    "#print(json.dumps(tone_analysis, indent=2))\n",
    "\n",
    "# On the JSON response, the tone is given for the entire document on the document_tone element \n",
    "# as well as for each sentence of the document on the sentences_tone element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sentences Tones\n",
    "sentences_tone_df = json_normalize(\n",
    "   data=tone_analysis[\"sentences_tone\"],\n",
    "    record_path=[\"tones\"],\n",
    "    meta=[\"sentence_id\", \"text\"],\n",
    ")\n",
    "sentences_tone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tone_df.groupby('tone_name')['score'].nunique().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TSLA stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version='v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ticker\n",
    "ticker = \"TSLA\"\n",
    "\n",
    "# Set timeframe to '1D'\n",
    "timeframe = \"1D\"\n",
    "\n",
    "# Set current date and the date from one month ago using the ISO format\n",
    "current_date = pd.Timestamp(datetime.now(), tz=\"America/New_York\").isoformat()\n",
    "past_date = pd.Timestamp(datetime.now()- timedelta(30), tz=\"America/New_York\").isoformat()\n",
    "\n",
    "# Get 4 weeks worth of historical data for AAPL\n",
    "df = api.get_barset(\n",
    "    ticker,\n",
    "    timeframe,\n",
    "    limit=None,\n",
    "    start=past_date,\n",
    "    end=current_date,\n",
    "    after=None,\n",
    "    until=None,\n",
    ").df\n",
    "\n",
    "\n",
    "# Drop Outer Table Level\n",
    "df = df.droplevel(axis=1, level=0)\n",
    "\n",
    "# Use the drop function to drop extra columns\n",
    "df = df.drop(columns=[\"open\", \"high\", \"low\", \"volume\"])\n",
    "\n",
    "# Since this is daily data, we can keep only the date (remove the time) component of the data\n",
    "df.index = df.index.date\n",
    "\n",
    "\n",
    "# Use the `pct_change` function to calculate daily returns of AAPL\n",
    "tsla_returns = df.pct_change().dropna()\n",
    "\n",
    "# Display sample data\n",
    "tsla_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_stock = pd.concat([tsla_returns, df], axis=1, join=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_stock.columns = [\"daily_returns\", \"close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_stock = tsla_stock.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_stock.columns = [\"date\", \"daily_returns\",\"close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.concat([words_tweets_df, tweet_sentiment_final],axis=1, join=\"inner\")\n",
    "sentiment_df = sentiment_df.drop(columns=[\"tweet\"])\n",
    "sentiment_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df[\"date\"] = pd.to_datetime(sentiment_df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_stock[\"date\"] = pd.to_datetime(tsla_stock[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df['date']=sentiment_df['date'].apply(lambda x: x.toordinal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_stock['date']=tsla_stock['date'].apply(lambda x: x.toordinal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(sentiment_df, tsla_stock, on=[\"date\"], how=\"inner\")\n",
    "new_df.set_index(\"date\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(columns=[\"Word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df[\"sentiment\"]\n",
    "X = new_df.loc[:,(new_df.columns != \"sentiment\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fitting Standard Scaler with the training data\n",
    "X_scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scaling data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating the decision tree classifier instance\n",
    "model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is trained with the scaled training data.\n",
    "model = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fitting the model, some predictions are made using the scaled testing data.\n",
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\",\"Actual 2\"], columns=[\"Predicted 0\", \"Predicted 1\",\"Predicted 3\"]\n",
    ")\n",
    "\n",
    "# Calculating the accuracy score\n",
    "acc_score = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# despite high accuracy, precision and recall for fraudulent loans are relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(\n",
    "    model, out_file=None, feature_names=X.columns, class_names=[\"0\", \"1\",\"2\"], filled=True\n",
    ")\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())\n",
    "\n",
    "# The tree is too deep. However, it can be observed that only one subtree grows.\n",
    "# This phenomenon occurs when the target classes are imbalanced.\n",
    "# Having imbalanced target classes is a common problem in classification machine learning algorithms when there is a disproportionate ratio of observations in each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run a grid search to find error rates for max_depths ranging 1-30\n",
    "grid_param_tree = {'criterion': ['gini', 'entropy'], 'max_depth': range(1, 31)}\n",
    "\n",
    "tree.set_param(random_state=0)\n",
    "\n",
    "grid_search_tree = ms.GridSearchCV(tree, grid_param_tree, scoring='accuracy', cv=5, n_jobs=-1, return_train_score=True)\n",
    "grid_search_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "\n",
    "# Create a Logistic Regression Model\n",
    "# Using a logistic regression model, we try to predict the class, purple or yellow, with the coordinates of a point.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Fit (train) or model using the training data\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, predictions)\n",
    "\n",
    "# it seems that the model does reasonably well.\n",
    "# While there are some false positives, the vast majority of data points are classified correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = [\"Positive\", \"Neutral\",\"Negative\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predictions = model.predict(new_df)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=new_df[\"positive\"], y=new_df[\"neutral\"], c=new_df[\"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(k, data):\n",
    "    # Initialize the K-Means model\n",
    "    model = KMeans(n_clusters=k, random_state=0)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(data)\n",
    "\n",
    "    # Predict clusters\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Create return DataFrame with predicted clusters\n",
    "    #data[\"class\"] = model.labels_\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow method\n",
    "# Looking for the best k\n",
    "\n",
    "inertia = []\n",
    "k = list(range(1, 11))\n",
    "\n",
    "for i in k:\n",
    "    km = KMeans(n_clusters=i, random_state=0)\n",
    "    km.fit(new_df)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# Define a DataFrame to plot the Elbow Curve using hvPlot\n",
    "elbow_data = {\"k\": k, \"inertia\": inertia}\n",
    "df_elbow = pd.DataFrame(elbow_data)\n",
    "df_elbow.hvplot.line(x=\"k\", y=\"inertia\", title=\"Elbow Curve\", xticks=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination Sampling SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df[\"sentiment\"]\n",
    "X = new_df.loc[:,(new_df.columns != \"sentiment\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Logistic regression model using random undersampled data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Balanced Accuracy Score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the imbalanced classification report\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test and Train\n",
    "__________\n",
    "\n",
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_sentiment_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweet_sentiment_final[\"tweet\"],\\\n",
    "                                                    tweet_sentiment_final[\"sentiment\"],\\\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train=tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test=tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing and applying TF-IDF\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = MultinomialNB()\n",
    "Naive.fit(tfidf_train,y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(tfidf_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_df[\"compound\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop('compound',axis=1)\n",
    "y = new_df['compound'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the training and testing data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=500, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop('compound',axis=1)\n",
    "y = new_df['compound'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the training and testing data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "# Calculating the accuracy score\n",
    "acc_score = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "_____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweet_sentiment_final[\"tweet\"],\\\n",
    "                                                    tweet_sentiment_final[\"sentiment\"],\\\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train=tfidf_vectorizer.fit_transform(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test=tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(tfidf_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(tfidf_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n",
    "#    inputs = Input(name='inputs',shape=[max_len])#step1\n",
    "#    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n",
    "#    layer = LSTM(64)(layer) #step3\n",
    "#    layer = Dense(256,name='FC1')(layer) #step4\n",
    "#    layer = Activation('relu')(layer) # step5\n",
    "#    layer = Dropout(0.5)(layer) # step6\n",
    "#    layer = Dense(1,name='out_layer')(layer) #step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n",
    "#    layer = Activation('sigmoid')(layer) #step5 but this time activation function is sigmoid for only one output.\n",
    "#    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
    "#    return model #function returning the value when we call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tensorflow_based_model() # here we are calling the function of created model\n",
    "# model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history=model.fit(X_train,y_train,batch_size=80,epochs=6, validation_split=0.1)# here we are starting the training of model by feeding the training data\n",
    "# print('Training finished !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Market Prediction and Forecasting using Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stacked LSTM model will predict the test data and plot the output then predict the future 30 days and plot the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "import pandas_datareader as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_df = pdr.get_data_tiingo('TSLA',api_key = '614ecbf1d11d1075beea94c5d46a05acb5163a82')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsla_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_df1 = tsla_df.reset_index()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tsla_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler #LSTM is sensitive to teh scale of teh data, so MinMax Scaler is imported\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "tsla_df1=scaler.fit_transform(np.array(tsla_df1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into train and test split\n",
    "training_size = int(len(tsla_df1)*0.65)\n",
    "test_size = len(tsla_df1) - training_size\n",
    "train_data, test_data = tsla_df1[0:training_size,:], tsla_df1[training_size:len(tsla_df1), :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0] # i=0, 0,1,2,3\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t, t+1, t+2, t+3, and Y=y+4\n",
    "time_step = 100\n",
    "x_train, y_train = create_dataset(train_data, time_step)\n",
    "x_test, ytest = create_dataset(test_data, time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape), print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input to be [samples, time-steps, features] which is required for LSTM\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] , 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the lstm model\n",
    "#from tesnforflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense\n",
    "#from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,1)))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_test,ytest), epochs=100, batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction and check performance metrics\n",
    "train_predict = model.predict(x_train)\n",
    "test_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform back to original form\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE performance metrics\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "math.sqrt(mean_squared_error(y_train,train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data RMSE\n",
    "math.sqrt(mean_squared_error(ytest,test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# shift train predictions for plotting\n",
    "look_back = 100\n",
    "trainPredictPlot = np.empty_like(tsla_df1)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(tsla_df1)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(tsla_df1)-1, :] = test_predict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(tsla_df1))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "441-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = test_data[341:].reshape(1,-1)\n",
    "x_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = test_data[341:].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_input = list(x_input)\n",
    "temp_input = temp_input[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# demonstrate prediction for next 10 days\n",
    "from numpy import array\n",
    "\n",
    "lst_output = []\n",
    "n_steps = 100\n",
    "i = 0\n",
    "while(i<3):\n",
    "    \n",
    "    if(len(temp_input)>100):\n",
    "        #print(temp_input)\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        print(\"{} day input {}\" .format(1, x_input))\n",
    "        x_input=x_input.reshape(1,-1)\n",
    "        x_input=x_input.reshape((1,n_steps,1))\n",
    "        #print(x_input)\n",
    "        yhat=model.predict(x_input, verbose=0)\n",
    "        print(\"{} day output {}\" .format(1, yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        l=i+1\n",
    "    else:\n",
    "        x_input=x_input.reshape((1,n_steps,1))\n",
    "        yhat=model.predict(x_input, verbose=0)    \n",
    "        print(yhat[0])\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        print(len(temp_input))\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "        \n",
    "print(lst_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_new = np.arrange(1,101)\n",
    "day_pred = np.arrange(101,131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tsla_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_df2 = tsla_df1.tolist()\n",
    "tsla_df2.extend(lst_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
